{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Po0jwny1PlC8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, BatchNormalization\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "  # Load MNIST & NIST dataset\n",
        "  digits_data_train = pd.read_csv(\"sample_data/mnist_train.csv\")\n",
        "  digits_data_test = pd.read_csv(\"sample_data/mnist_test.csv\")\n",
        "  letters_dataset = pd.read_csv(\"sample_data/A_Z Handwritten Data.csv\")\n",
        "  digits_data = pd.concat([digits_data_train, digits_data_test], ignore_index=True)\n",
        "  # Rename correct label column name to label\n",
        "  digits_data.rename(columns={'0':'label'}, inplace=True)\n",
        "  letters_dataset.rename(columns={'0':'label'}, inplace=True)\n",
        "\n",
        "  # Select 1000 samples from each class\n",
        "  digits_data = digits_data.groupby('label').head(1000)\n",
        "  letters_dataset = letters_dataset.groupby('label').head(1000)\n",
        "\n",
        "  # Split data into X and Y for each type\n",
        "  Y1 = digits_data['label']\n",
        "  X1 = digits_data.drop('label', axis=1)\n",
        "  Y2 = letters_dataset[\"label\"]\n",
        "  X2 = letters_dataset.drop(\"label\", axis=1)\n",
        "\n",
        "  # Split data into train and test set\n",
        "  x_train1, x_test1, y_train1, y_test1 = train_test_split(X1, Y1, train_size=0.9)\n",
        "  x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, Y2, train_size=0.9)\n",
        "\n",
        "  # Convert into numpy array to ease preprocessing\n",
        "  x_train1 = x_train1.to_numpy()\n",
        "  x_test1 = x_test1.to_numpy()\n",
        "  x_train2 = x_train2.to_numpy()\n",
        "  x_test2 = x_test2.to_numpy()\n",
        "\n",
        "  # Add ten to match number of classes for one-hot\n",
        "  y_train2 = y_train2 + 10\n",
        "  y_test2 = y_test2 + 10\n",
        "\n",
        "  # Convert Y into one-hot vectors\n",
        "  y_train1 = to_categorical(y_train1, num_classes=36)\n",
        "  y_test1 = to_categorical(y_test1, num_classes=36)\n",
        "  y_train2 = to_categorical(y_train2, num_classes=36)\n",
        "  y_test2 = to_categorical(y_test2, num_classes=36)\n",
        "\n",
        "  # Reshape Xs into CNN input dimension\n",
        "  x_train1 = x_train1.reshape(x_train1.shape[0], 28, 28, 1)\n",
        "  x_test1 = x_test1.reshape(x_test1.shape[0], 28, 28, 1)\n",
        "  x_train2 = x_train2.reshape(x_train2.shape[0], 28, 28, 1)\n",
        "  x_test2 = x_test2.reshape(x_test2.shape[0], 28, 28, 1)\n",
        "\n",
        "  # Combine each X and Y from each dataset\n",
        "  x_train = np.concatenate((x_train1, x_train2), axis=0)\n",
        "  x_test = np.concatenate((x_test1, x_test2), axis=0)\n",
        "  y_train = np.concatenate((y_train1, y_train2), axis=0)\n",
        "  y_test = np.concatenate((y_test1, y_test2), axis=0)\n",
        "\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "def preprocessing(x_train, x_test):\n",
        "  x_train = x_train / 255\n",
        "  x_test = x_test / 255\n",
        "  return x_train, x_test"
      ],
      "metadata": {
        "id": "J2UFnfyZPs81"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, x_test, y_train, y_test = load_data()\n",
        "(x_train, x_test) = preprocessing(x_train, x_test)\n",
        "# Initialize Keras image generator\n",
        "generator = ImageDataGenerator(rotation_range=10, zoom_range=0.1, width_shift_range=0.1, height_shift_range=0.1)\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=3, input_shape=(28, 28, 1), activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=32, kernel_size=3, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=32, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=64, kernel_size=3, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Conv2D(filters=64, kernel_size=5, strides=2, padding=\"same\", activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Dropout(0.4),\n",
        "    Conv2D(128, kernel_size=4, activation=\"relu\"),\n",
        "    BatchNormalization(),\n",
        "    Flatten(),\n",
        "    Dropout(0.4),\n",
        "    Dense(36, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(generator.flow(x_train, y_train, batch_size=64), epochs=30, steps_per_epoch=x_train.shape[0] // 64, validation_data=(x_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5PLMum0Qurq",
        "outputId": "9ff70f99-9368-477a-a830-cdd0d542642c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 299ms/step - accuracy: 0.3876 - loss: 2.3860 - val_accuracy: 0.4950 - val_loss: 1.7814\n",
            "Epoch 2/30\n",
            "\u001b[1m  1/506\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 210ms/step - accuracy: 0.8750 - loss: 0.5450"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.8750 - loss: 0.5450 - val_accuracy: 0.5061 - val_loss: 1.7075\n",
            "Epoch 3/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 295ms/step - accuracy: 0.8230 - loss: 0.5688 - val_accuracy: 0.9253 - val_loss: 0.1986\n",
            "Epoch 4/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8125 - loss: 0.7359 - val_accuracy: 0.9258 - val_loss: 0.1974\n",
            "Epoch 5/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m152s\u001b[0m 300ms/step - accuracy: 0.8661 - loss: 0.4147 - val_accuracy: 0.9356 - val_loss: 0.1922\n",
            "Epoch 6/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.9062 - loss: 0.3371 - val_accuracy: 0.9347 - val_loss: 0.1924\n",
            "Epoch 7/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 297ms/step - accuracy: 0.8849 - loss: 0.3429 - val_accuracy: 0.9350 - val_loss: 0.1896\n",
            "Epoch 8/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9219 - loss: 0.1744 - val_accuracy: 0.9369 - val_loss: 0.1873\n",
            "Epoch 9/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 294ms/step - accuracy: 0.8956 - loss: 0.3109 - val_accuracy: 0.9367 - val_loss: 0.1741\n",
            "Epoch 10/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 8ms/step - accuracy: 0.8906 - loss: 0.3230 - val_accuracy: 0.9375 - val_loss: 0.1702\n",
            "Epoch 11/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 294ms/step - accuracy: 0.9098 - loss: 0.2712 - val_accuracy: 0.9381 - val_loss: 0.1738\n",
            "Epoch 12/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8594 - loss: 0.2911 - val_accuracy: 0.9375 - val_loss: 0.1748\n",
            "Epoch 13/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 292ms/step - accuracy: 0.9136 - loss: 0.2619 - val_accuracy: 0.9331 - val_loss: 0.1687\n",
            "Epoch 14/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9062 - loss: 0.2438 - val_accuracy: 0.9347 - val_loss: 0.1665\n",
            "Epoch 15/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 295ms/step - accuracy: 0.9171 - loss: 0.2475 - val_accuracy: 0.9450 - val_loss: 0.1483\n",
            "Epoch 16/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8750 - loss: 0.3714 - val_accuracy: 0.9458 - val_loss: 0.1496\n",
            "Epoch 17/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m197s\u001b[0m 291ms/step - accuracy: 0.9181 - loss: 0.2389 - val_accuracy: 0.9514 - val_loss: 0.1412\n",
            "Epoch 18/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9375 - loss: 0.1428 - val_accuracy: 0.9500 - val_loss: 0.1425\n",
            "Epoch 19/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 295ms/step - accuracy: 0.9209 - loss: 0.2288 - val_accuracy: 0.9533 - val_loss: 0.1288\n",
            "Epoch 20/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.9531 - loss: 0.1155 - val_accuracy: 0.9536 - val_loss: 0.1289\n",
            "Epoch 21/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 292ms/step - accuracy: 0.9237 - loss: 0.2217 - val_accuracy: 0.9561 - val_loss: 0.1183\n",
            "Epoch 22/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.9062 - loss: 0.2212 - val_accuracy: 0.9575 - val_loss: 0.1170\n",
            "Epoch 23/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 286ms/step - accuracy: 0.9287 - loss: 0.2117 - val_accuracy: 0.9475 - val_loss: 0.1506\n",
            "Epoch 24/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9219 - loss: 0.3370 - val_accuracy: 0.9472 - val_loss: 0.1513\n",
            "Epoch 25/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 286ms/step - accuracy: 0.9264 - loss: 0.2107 - val_accuracy: 0.9556 - val_loss: 0.1352\n",
            "Epoch 26/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 10ms/step - accuracy: 0.9531 - loss: 0.1117 - val_accuracy: 0.9550 - val_loss: 0.1364\n",
            "Epoch 27/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 283ms/step - accuracy: 0.9302 - loss: 0.2016 - val_accuracy: 0.9503 - val_loss: 0.1334\n",
            "Epoch 28/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9062 - loss: 0.1716 - val_accuracy: 0.9497 - val_loss: 0.1320\n",
            "Epoch 29/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m199s\u001b[0m 284ms/step - accuracy: 0.9285 - loss: 0.2056 - val_accuracy: 0.9453 - val_loss: 0.1369\n",
            "Epoch 30/30\n",
            "\u001b[1m506/506\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9375 - loss: 0.1320 - val_accuracy: 0.9469 - val_loss: 0.1349\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('handwriting_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYwHauKRb448",
        "outputId": "88cb9c8b-c297-4d1b-dcf2-aa82e5a8a0a8"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    }
  ]
}